# PyBCI Examples

This folder holds multiple scripts illustrating the functions and configurations available within the PyBCI package.

NOTE: all the examples shown that are not in a dedicated folder work with the createPseudoDevice variable set to True when instantiating PyBCI(). If using with own LSL capable hardware you may need to adjust the scripts accordingly, namely set `createPseudoDevice=False`.

| Example File | Description |
|--------------|-------------|
| [ArduinoHandGrasp/](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/ArduinoHandGrasp/) | Folder contains an LSL marker creator in [MarkerMaker.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/ArduinoHandGrasp/MarkerMaker.py) using PyQt5 as an on screen text stimulus, illustrates how LSL markers can be used to train. [ServoControl.ino](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/ArduinoHandGrasp/ServoControl/ServoControl.ino) is designed for an arduino uno which controls 5 servo motors, each of which control the position of an indidividual finger for a 3D printed hand which can be controlled via serial commands. There is also a [Myoware Muscle Sensor](https://myoware.com/products/muscle-sensor/) attached to analog pin A0 being read continuously over the serial connection. [ArduinoToLSL.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/ArduinoHandGrasp/ArduinoToLSL.py) is used to send and receive serial data to and from the arduino, whilst pushing the A0 data to an LSL outlet which is classified in [testArduinoHand.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/ArduinoHandGrasp/testArduinoHand.py), whilst simultaneously receiving a marker stream from testArduinoHand.py to inform which hand position to do.|
| [MultimodalPupilLabsEEG/](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/MultimodalPupilLabsEEG) | Advanced example illustrating two devices, pupil labs gaze device stream wth custom feature extractor class and Hull University ioBio EEG device, specifically channels Fp1 and Fp2. The PupilLabsRightLeftEyeClose folder has a youtube video illustrating the multimodal example in action. |
| [PupilLabsRightLeftEyeClose/](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/PupilLabsRightLeftEyeClose/) | Folder contains example basic pupil labs example as LSL input device, classifying left and right eye closed with a custom extractor class. RightLeftMarkers.py uses tkinter to generate visual on-screen stimuli for only right, left or both eyes open, sends same onscreen stimuli as LSL markers, ideal for testing pupil-labs eyes classifier test. bciGazeExample.py Illustrates how a 'simple' custom pupil-labs feature extractor class can be passed for the gaze data, where the mean pupil diameter is taken for each eye and both eyes and used as feature data, where nans for no confidence are set to a value of 0. |
| [testEpochTimingsConfig.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/testEpochTimingsConfig.py) | Simple example showing custom global epoch settings  changed on initialisation. Instead of epoching data from 0 to 1 second after the marker we take it from 0.5 seconds before to 0.5 seconds after the marker. |
| [testPytorch.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/testPytorch.py) | Provides an example of how to use a Pytorch Neural net Model as the classifier. (testRaw.py also has a Pytorch example with a C-NN). |
| [testRaw.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/testRaw.py) | This example shows how raw time series across multiple channels can be used as an input by utilising a custom feature extractor class, combined with a custom C-NN Pytorch model when initialising PyBCI. The raw data from the data receiver thread comes in the form [samples, channels], the data receiver threads slice data based on relative timestamps meaning depending on the devices frequency for pushing LSL chunks can vary the number of samples received in the buffer for each window, to mitigate this a desired length is set and data should be trimmed based on the expected data for the created model. Multiple channels are also dropped (with the PsuedoLSLSreamGenerator in mind) to save computational complexity as raw time series over large windows can give a lot of parameters for the neural net to train.|
| [testSimple.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/testSimple.py) | Provides the simplest setup, where no specific streams or epoch settings are given, all default to sklearn SVM classifier and GeneralEpochSettings. |
| [testSklearn.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/testSklearn.py) | Similar to testSimple.py, but allows a custom sklearn classifier to be used. It sets individual time windows and configures data stream channels, epoch window settings, and machine learning settings before connecting to BCI and switching between training and test modes. |
| [testTensorflow.py](https://github.com/LMBooth/pybci/blob/main/pybci/Examples/testTensorflow.py) | Similar to testSimple.py, but allows a custom TensorFlow model to be used. It establishes a connection to BCI, starts training on received epochs, checks the classifier's accuracy, and then switches to test mode to predict the current marker. |
